{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYq6Ix35Z9CB"
   },
   "source": [
    "# Arabic Dialect Classification Using Explainable AI and Semantic Vector Embeddings\n",
    "\n",
    "This notebook implements a dialect classification system for Arabic text using the MADAR dataset, which contains sentences in 26 different Arabic dialects (including Modern Standard Arabic).\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "- **Task**: Classify Arabic sentences according to their local dialect\n",
    "- **Model**: AraBERT (Arabic BERT)\n",
    "- **Explainability**: SHAP and LIME for model interpretation\n",
    "- **Dataset**: MADAR Corpus (Multi-Arabic Dialect Applications and Resources)\n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    "1. Data Loading and Exploration\n",
    "2. Data Preprocessing\n",
    "3. AraBERT Model Setup\n",
    "4. Model Training\n",
    "5. Model Evaluation\n",
    "6. Model Interpretation with XAI\n",
    "7. Visualization and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEguESgyZ9CE"
   },
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddbXV9lBZ9CF",
    "outputId": "e0011503-891c-4aea-be28-83d9492ef64c"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets pandas numpy matplotlib seaborn scikit-learn torch lime shap arabert pyarabic farasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xk31lfTIZ9CH"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "import lime\n",
    "import lime.lime_text\n",
    "import shap\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU70gBOOZ9CH"
   },
   "source": [
    "## 2. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "Qd17lLSUZ9CH",
    "outputId": "41c6d2ad-44b4-443e-b89a-ae11d2f881dd"
   },
   "outputs": [],
   "source": [
    "# Load the MADAR dataset\n",
    "df = pd.read_csv('MADAR_Corpus_Combined.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4lBfPt9-Z9CI",
    "outputId": "88e06816-f031-4673-e5d2-dd26707e05d3"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2ZPCC5RjZ9CI",
    "outputId": "2fd4e37f-c737-4f36-ba53-29ba46a543d3"
   },
   "outputs": [],
   "source": [
    "# Explore the distribution of dialects\n",
    "dialect_counts = df['lang'].value_counts()\n",
    "print(f\"Number of unique dialects: {len(dialect_counts)}\")\n",
    "print(\"\\nDialect distribution:\")\n",
    "print(dialect_counts)\n",
    "\n",
    "# Visualize dialect distribution\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(x=dialect_counts.index, y=dialect_counts.values)\n",
    "plt.title('Distribution of Arabic Dialects in the Dataset')\n",
    "plt.xlabel('Dialect')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "id": "3nZtHhEoZ9CI",
    "outputId": "d5d62802-ac47-4cd2-a885-9816fec85440"
   },
   "outputs": [],
   "source": [
    "# Explore sentence lengths\n",
    "df['sent_length'] = df['sent'].apply(len)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df['sent_length'], bins=50)\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.xlabel('Length (characters)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average sentence length: {df['sent_length'].mean():.2f} characters\")\n",
    "print(f\"Minimum sentence length: {df['sent_length'].min()} characters\")\n",
    "print(f\"Maximum sentence length: {df['sent_length'].max()} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CY1u8mDZ9CI"
   },
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epzgCsYrZ9CI",
    "outputId": "c2953301-dced-4728-c41a-ec530fbcfcee"
   },
   "outputs": [],
   "source": [
    "# Create a mapping from dialect labels to numeric indices\n",
    "dialects = sorted(df['lang'].unique())\n",
    "dialect_to_id = {dialect: idx for idx, dialect in enumerate(dialects)}\n",
    "id_to_dialect = {idx: dialect for dialect, idx in dialect_to_id.items()}\n",
    "\n",
    "# Add numeric labels to the dataframe\n",
    "df['label'] = df['lang'].map(dialect_to_id)\n",
    "\n",
    "# Display the mapping\n",
    "print(\"Dialect to ID mapping:\")\n",
    "for dialect, idx in dialect_to_id.items():\n",
    "    print(f\"{dialect}: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AcTVqcTzZ9CJ",
    "outputId": "87e08884-8dfc-485e-d42c-2a98cda387dd"
   },
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "# First, separate by split column if it exists\n",
    "if 'split' in df.columns:\n",
    "    train_df = df[df['split'].str.contains('train')].copy()\n",
    "    val_df = df[df['split'].str.contains('dev')].copy()\n",
    "    test_df = df[df['split'].str.contains('test')].copy()\n",
    "\n",
    "    print(f\"Training set size: {len(train_df)}\")\n",
    "    print(f\"Validation set size: {len(val_df)}\")\n",
    "    print(f\"Test set size: {len(test_df)}\")\n",
    "else:\n",
    "    # If no split column, create our own splits\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "    print(f\"Training set size: {len(train_df)}\")\n",
    "    print(f\"Validation set size: {len(val_df)}\")\n",
    "    print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AvQkubeZ9CJ"
   },
   "source": [
    "## 4. AraBERT Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368,
     "referenced_widgets": [
      "26da740cf6524b89934bce741ad23942",
      "4425bd823a3549319234228cff46cfea",
      "4bbd8b9cfc494d269e2711b97956a966",
      "18aa8b02eac64bc8a2b63cec080132e6",
      "8a34d20073e5495488d9b9ebf7d4ab19",
      "21017dcc8de34deaa995a34490e91800",
      "290d489d20cf41c4a8a83c1aaf413014",
      "db1be4d50868467091be9a30168144cc",
      "a17266fec5f446ed93c3df51f6c297aa",
      "6ed83256e7254c13b97f9bcdcf416aa0",
      "2c2cce25bf234ce4bd02d0c110adeb46",
      "f7a94edb320c4e25a8b3513938f79fc9",
      "8b1eb1d6073d452cb6ef67ef9df29c9d",
      "a3438f43eb4c4c9196a3a98c0750d068",
      "8d1b2cfe8729409e8d7156e3a37ae636",
      "6888e160568945e19e21fe0d8c2ed6df",
      "99717b17dcce44afb8731033b6528c7b",
      "a881caaeaaca47359f25e4e91230c86a",
      "8f85808ada5f4d778c22e7cebc3c3535",
      "e451aa489c7a4c2fb5b8d3169d14aedf",
      "1ee82b0377e342f0826112b668d24bf4",
      "f46d35399f0943479752f2b33a5cd7bf",
      "c0a5785539e7400fa44cb7c561065ec8",
      "dee56b99774548d298566092e776147d",
      "fbc846819e834d8c8730a9e4003b78cf",
      "c0bccaba350a42c48ab153cdf506b6f6",
      "641a6d033746451ab59f6b7b0e0436c2",
      "54d529743ab042beab80694e0d0baa55",
      "7cb7a0e849ae46c599a19e0aa4f6f8e2",
      "d3c650723dc84aaaa0f9e18c72e57813",
      "18e88a798d6e43fa8ecfaa1148f5e54c",
      "16a030bfc4c64c028c8accde341f9ff2",
      "5f1b054b30b041a1a25013a2a740cea8",
      "6588c86e7e514f498587d21a6068ece4",
      "8ab054ec9b6e4a079b033fa69c317f94",
      "374a07bf02b54ec39d58a531502d3cce",
      "b078e7dac55449478b4a3d70cce04cd8",
      "0359e9d3cdce425d9c3fc1f1d9f487cb",
      "b3da9b06e6254016a555fa55ce8efa90",
      "6e901b9761c6415d89ef3352edc485a2",
      "a2c7b315ec914b698b226edfd19f928c",
      "cfe46a4d7e4142d080996635ecf0b624",
      "285201ddca29463ca8ad1aef18140915",
      "d7aa069cc4ae410d9ad999fc20b813c8",
      "ba544de4cc2544059f0a17a9f4e9641e",
      "d46b49cc587b453d9bb2c0bd35f95777",
      "dc109796c7734576b2cafb2f11b42cc1",
      "589e4fb3de6a4487964d1c6f48a55459",
      "bf3b10b400b543d6a423af235b1989a6",
      "175fecef7f1d405583971ae92c1f51d8",
      "c93b14bdaada4f4a80208b52c96b50ef",
      "81f274f910464eb28f25e58e55ca4193",
      "6f40b479b7a0452ea944ba89804c299e",
      "98f65e403c10449e96cc358bd3ec2769",
      "ae7b05e0f5d449c780080b5fe0d7302b",
      "6ea079f7fbbe43a2afdffea1b62df575",
      "93146d6215214aa4851b18f22d20e19e",
      "764bb033eb4449dc910dae5fb5c92c35",
      "7931fbd3e7ec4d4aaa9fed8992dafb37",
      "2651a32065e1444b860a21d8e0dcd88b",
      "4f92330da1eb4d5dac782cf4690d9836",
      "387dde817f8247c592c2b60d06a26154",
      "2e17408d90e54da98e3890bad7d64636",
      "b005981e76fa41eb98925840ee986ad6",
      "b3a9eb470b32440d9ca6442fbe710a95",
      "c48bdb317bd044eb9d8d6c12fa52103b"
     ]
    },
    "id": "LGuT9ckAZ9CJ",
    "outputId": "9c604d98-24a5-4683-9e29-4318416cc9a6"
   },
   "outputs": [],
   "source": [
    "# Load AraBERT tokenizer and model\n",
    "model_name = \"aubmindlab/bert-base-arabertv01\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create a model with the correct number of labels (dialects)\n",
    "num_labels = len(dialects)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOybXe5PZ9CJ"
   },
   "outputs": [],
   "source": [
    "# Create a custom dataset class for our Arabic dialect data\n",
    "class ArabicDialectDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  truncation=True,\n",
    "                                  padding='max_length',\n",
    "                                  max_length=self.max_length,\n",
    "                                  return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "navWJVvgZ9CJ"
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ArabicDialectDataset(train_df['sent'].tolist(), train_df['label'].tolist(), tokenizer)\n",
    "val_dataset = ArabicDialectDataset(val_df['sent'].tolist(), val_df['label'].tolist(), tokenizer)\n",
    "test_dataset = ArabicDialectDataset(test_df['sent'].tolist(), test_df['label'].tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60CKxWshZ9CK"
   },
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25z7Z05hZ9CK"
   },
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sro3jhNwZ9CK"
   },
   "outputs": [],
   "source": [
    "# Define compute_metrics function for evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDaE514bZ9CK"
   },
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "ze6Ea8uqZ9CL",
    "outputId": "abeda30c-75c2-436d-905c-b16465ebb51d"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xcZiDM8Z9CL"
   },
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "byVc_SftZ9CL",
    "outputId": "26aef7dd-f2f2-40c1-e4a9-04d1f7611335"
   },
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_WTV49MZ9CL"
   },
   "outputs": [],
   "source": [
    "# Get predictions for the test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Convert numeric predictions back to dialect labels\n",
    "pred_dialects = [id_to_dialect[pred] for pred in preds]\n",
    "true_dialects = [id_to_dialect[label] for label in labels]\n",
    "\n",
    "# Create a classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_dialects, pred_dialects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aMCM-LyqZ9CL",
    "outputId": "c10efa06-7e66-4fb4-d65a-1f21cb3acb5a"
   },
   "outputs": [],
   "source": [
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=dialects, yticklabels=dialects)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrY3HRpbZ9CO"
   },
   "source": [
    "## 7. Model Deployment and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASqcS_yAZ9CO"
   },
   "outputs": [],
   "source": [
    "# Save the trained model and tokenizer\n",
    "model_save_path = './arabic_dialect_model'\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Save the dialect mappings\n",
    "import json\n",
    "with open(f\"{model_save_path}/dialect_mappings.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'dialect_to_id': dialect_to_id,\n",
    "        'id_to_dialect': {str(k): v for k, v in id_to_dialect.items()}  # Convert int keys to strings for JSON\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ZbjlXCVR5HZe",
    "outputId": "4363db3e-802b-4ec9-f390-fb010f390bf2"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('arabic_dialect_model', 'zip', './arabic_dialect_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "6wmf5t9P5Ioh",
    "outputId": "adcb3086-70fd-43d8-9ac3-5b6fa47f4421"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('arabic_dialect_model.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "in9s-4RdZ9CP"
   },
   "outputs": [],
   "source": [
    "# Create a simple inference function\n",
    "def predict_dialect(text):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get predicted class\n",
    "    pred_idx = outputs.logits.argmax(-1).item()\n",
    "    pred_dialect = id_to_dialect[pred_idx]\n",
    "\n",
    "    # Get probabilities\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    # Get top 3 predictions\n",
    "    top_indices = probs.argsort()[-3:][::-1]\n",
    "    top_dialects = [id_to_dialect[idx] for idx in top_indices]\n",
    "    top_probs = probs[top_indices]\n",
    "\n",
    "    return {\n",
    "        'text': text,\n",
    "        'predicted_dialect': pred_dialect,\n",
    "        'confidence': float(probs[pred_idx]),\n",
    "        'top_predictions': [{'dialect': d, 'probability': float(p)} for d, p in zip(top_dialects, top_probs)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2BlCH3X_Z9CQ",
    "outputId": "c5415ae3-1218-4bf4-f2b9-9518c6433c68"
   },
   "outputs": [],
   "source": [
    "# Test the inference function with some examples\n",
    "test_examples = [\n",
    "    \"أنا رايح على البيت\",  # I'm going home\n",
    "    \"شلونك اليوم؟\",       # How are you today?\n",
    "    \"بدي أروح عالسوق\"      # I want to go to the market\n",
    "]\n",
    "\n",
    "for example in test_examples:\n",
    "    result = predict_dialect(example)\n",
    "    print(f\"\\nText: {result['text']}\")\n",
    "    print(f\"Predicted dialect: {result['predicted_dialect']} (confidence: {result['confidence']:.4f})\")\n",
    "    print(\"Top predictions:\")\n",
    "    for pred in result['top_predictions']:\n",
    "        print(f\"  {pred['dialect']}: {pred['probability']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAi3CUxI5mty"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = './arabic_dialect_model'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Ensure model is on the correct device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Load dialect mappings\n",
    "with open(f\"{model_path}/dialect_mappings.json\", 'r') as f:\n",
    "    mappings = json.load(f)\n",
    "    dialect_to_id = mappings['dialect_to_id']\n",
    "    id_to_dialect = {int(k): v for k, v in mappings['id_to_dialect'].items()}  # Convert keys back to int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khYBG_pb5t0h"
   },
   "outputs": [],
   "source": [
    "def predict_dialect(text):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get predicted class\n",
    "    pred_idx = outputs.logits.argmax(-1).item()\n",
    "    pred_dialect = id_to_dialect[pred_idx]\n",
    "\n",
    "    # Get probabilities\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    # Get top 3 predictions\n",
    "    top_indices = probs.argsort()[-3:][::-1]\n",
    "    top_dialects = [id_to_dialect[idx] for idx in top_indices]\n",
    "    top_probs = probs[top_indices]\n",
    "\n",
    "    return {\n",
    "        'text': text,\n",
    "        'predicted_dialect': pred_dialect,\n",
    "        'confidence': float(probs[pred_idx]),\n",
    "        'top_predictions': [{'dialect': d, 'probability': float(p)} for d, p in zip(top_dialects, top_probs)]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ASc_axAw5xHG",
    "outputId": "76a1f1b3-1a1e-45b2-f084-9ec7c7b59417"
   },
   "outputs": [],
   "source": [
    "predict_dialect(\"إزّيك يا باشا؟\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI_R5fTjZ9CQ"
   },
   "source": [
    "## 8. Conclusion and Future Work\n",
    "\n",
    "In this notebook, we've built a comprehensive Arabic dialect classification system using AraBERT and explainable AI techniques. The model can accurately classify Arabic text into 26 different dialects from across the Arab world.\n",
    "\n",
    "### Key Achievements:\n",
    "\n",
    "1. **Dialect Classification**: Successfully trained a model to distinguish between 26 Arabic dialects.\n",
    "2. **Model Interpretability**: Used LIME and SHAP to explain model predictions.\n",
    "3. **Attention Visualization**: Visualized attention patterns to understand what the model focuses on.\n",
    "4. **Deployment Ready**: Created a simple inference function for practical use.\n",
    "\n",
    "### Future Work:\n",
    "\n",
    "1. **Fine-tuning**: Further optimize the model with hyperparameter tuning.\n",
    "2. **Data Augmentation**: Expand the dataset with more examples for underrepresented dialects.\n",
    "3. **Web Interface**: Develop a user-friendly web application for dialect classification.\n",
    "4. **Dialect Features**: Analyze specific linguistic features that distinguish different dialects.\n",
    "5. **Cross-Dialect Translation**: Extend the model for dialect-to-dialect or dialect-to-MSA translation.\n",
    "\n",
    "This project demonstrates the power of combining modern NLP techniques with explainable AI to create transparent and interpretable models for linguistic tasks."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "finalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
